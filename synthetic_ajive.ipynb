{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe81442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms.v2 as T\n",
    "import tqdm\n",
    "import wandb\n",
    "import cv2\n",
    "import skdim\n",
    "import torchvision\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"python_packages\"))\n",
    "from python_packages import *\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6b91d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/jazav7774/Library/CloudStorage/OneDrive-UiTOffice365/UiT/FYS-8603/artifacts/model-0l4ncfvk:v0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "model_weights_task1 = api.artifact('hzavadil98/Synthetic data/model-hgjnw6q6:v0', type='model')\n",
    "model_weights_task2 = api.artifact('hzavadil98/Synthetic data/model-0l4ncfvk:v0', type='model')\n",
    "model_weights_task1.download()\n",
    "model_weights_task2.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d788209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = TwoViewCNN.load_from_checkpoint('artifacts/model-hgjnw6q6:v0/model.ckpt')\n",
    "model_2 = TwoViewCNN.load_from_checkpoint('artifacts/model-0l4ncfvk:v0/model.ckpt')\n",
    "train_transform = T.Compose(\n",
    "   [\n",
    "       T.RandomHorizontalFlip(0.5),\n",
    "       T.RandomVerticalFlip(0.5),\n",
    "       T.GaussianNoise(0.1, 0.1),\n",
    "   ]\n",
    ")\n",
    "train_transform = None\n",
    "dataloader = Synthetic_2v_Dataloader(\n",
    "   n_samples=[3000, 1000, 1000], train_transform=train_transform, transform=None, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bb4c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing dataset: 100%|██████████| 94/94 [01:09<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_processed(model, dataloader):\n",
    "    model.to('mps')\n",
    "    model.eval()\n",
    "    all_in_features = []\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc=\"Featurizing dataset\"):\n",
    "            x, y1, y2 = batch\n",
    "            y = y1 if model.task == 1 else y2\n",
    "            x = [item.to('mps') for item in x]\n",
    "            in_features = [resnext(x_i) for resnext, x_i in zip(model.resnexts, x)]\n",
    "            all_in_features.append(in_features)\n",
    "            all_logits.append(model.fc(torch.cat(in_features, dim=1)))\n",
    "            all_labels.append(y)\n",
    "        all_in_features = [torch.cat([f[i] for f in all_in_features]).cpu().numpy() for i in range(2)]\n",
    "        all_logits = torch.cat(all_logits, dim=0).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels, dim=0).cpu().numpy()\n",
    "    return all_in_features, all_logits, all_labels\n",
    "\n",
    "task1_train_features_1, task1_train_logits_1, task1_train_labels_1 = get_dataset_processed(model_1, dataloader.train_dataloader(shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d043ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(task1_train_logits_1, task1_train_logits)\n",
    "np.sum(task1_train_labels_1 == task1_train_labels)\n",
    "np.allclose(task1_train_features_1[0], task1_train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47915a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadr = dataloader.train_dataloader()\n",
    "batch = next(iter(loadr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3c41e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9511, -2.6446,  0.9064],\n",
       "        [ 3.0826, -1.6011, -0.6030],\n",
       "        [-1.3275,  3.0731, -2.1125],\n",
       "        [ 2.3570, -0.9101, -0.8613],\n",
       "        [ 1.0499, -0.5063, -0.4077],\n",
       "        [ 3.4978, -0.8116, -1.4220],\n",
       "        [-2.5832, -1.6507,  3.0610],\n",
       "        [-2.0324,  1.4139, -0.1169],\n",
       "        [-1.5866,  2.2817, -1.4286],\n",
       "        [-3.0439,  1.0615,  1.7949],\n",
       "        [-1.5645,  0.4295,  0.7384],\n",
       "        [ 1.4326,  0.2850, -1.9699],\n",
       "        [-2.4162, -0.5444,  2.7665],\n",
       "        [ 2.7365, -1.8712, -0.1407],\n",
       "        [-0.3608,  0.1120, -0.2543],\n",
       "        [-0.8106,  1.0472, -1.1978],\n",
       "        [-3.3814,  1.6017,  2.3988],\n",
       "        [-1.6339,  3.1603, -2.3954],\n",
       "        [ 1.4509,  0.3348, -1.9037],\n",
       "        [-1.5839, -1.4920,  2.2109],\n",
       "        [ 0.0810,  0.3365, -0.4521],\n",
       "        [ 1.7463, -3.2812,  1.3192],\n",
       "        [-1.2392, -0.4792,  0.9002],\n",
       "        [-3.5499,  3.3755,  0.3926],\n",
       "        [-2.6563,  1.5992,  0.4666],\n",
       "        [-1.5535,  1.3608, -0.0809],\n",
       "        [-0.9517, -0.6679,  1.2819],\n",
       "        [ 0.0122, -3.0654,  2.3322],\n",
       "        [ 0.7263, -3.7897,  2.1205],\n",
       "        [ 0.2085, -2.0591,  1.4064],\n",
       "        [ 3.7828, -2.7350, -0.1047],\n",
       "        [ 0.4609, -1.5148,  0.8917]], device='mps:0',\n",
       "       grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.to('mps')\n",
    "model_1.device\n",
    "batch = [[batch[0][0].to('mps'), batch[0][1].to('mps')], batch[1].to('mps'), batch[2].to('mps')]\n",
    "model_1.predict_step(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
